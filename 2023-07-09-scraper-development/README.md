# 재무제표 스크래퍼 개발 목적

재무제표에서 주요 지표를 쉽게 조회할 수 있는 오픈API를 만들고자 했다. 처음에는 금융감독원에서 공식적으로 제공하는 API를 활용해 DB에 데이터를 채우려고했다. 하지만 공식 제공 API는 여러 한계가 있었다. 2016년 이후의 금융사를 제외한 상장기업의 데이터만 제공했다. 또한 제공되는 계정과목도 제한되어 있었다. 더구나 기업에 따라 매출액이나 당기순이익 등이 제대로 제공되지 않는 경우도 꽤나 있었다. 그래서 스크래핑을 통해서 직접 원하는 데이터를 보다 자유롭게 긁어와 DB를 채우기로 했다.

# 스크래핑 대상이 되는 URL 주소 구하기

공시코드를 알면 URL의 쿼리 파라미터를 살짝 바꾸어 기업 공시의 entry point를 바로 얻을 수 있었다. 특정 공시코드를 얻는 것은 어렵지 않았다. DART API를 통해 특정 기업이, 특정 기간 동안 제출한 모든 정기공시 목록과 공시 코드를 얻을 수 있었다.

보고서 중에서도 재무상태표, 손익계산서 등이 있는 재무제표가 있는 주소를 얻기 위해서는 각 공시에 바인딩된 dcmNo라는 것과, 재무제표에 해당하는 목차의 elemId라는 것을 알아야 했다. elemId의 경우 html에도 이미 박혀있는 것이었기 때문에 얻기가 어렵지는 않았다. 하지만 dcmNo의 경우 자바스크립트가 동작하고 나서야 html 상에서 조회할 수 있는 데이터였다. 따라서 HTTP request의 결과에서 바로 얻기 어려웠다. 검색을 해보니 백그라운드에서 JS를 작동시켜 데이터를 얻는 방식이 있었다. 하지만 다소 복잡해보이기도 하고 더 좋은 방식이 있지 않을까 하는 생각이 들어서 보고서의 html을 뒤져봤다.

월척이다! dcmNo가 html의 인라인 자바스크립트에 들어있는 것을 발견했다. 운 좋게 목차의 elemId도 트리 형태로 들어있었다. 해당 자바스크립트 코드 뒤쪽을 살짝 바꾸어 dcmNo와 elemId 데이터를 반환하는 함수로 바꾸었다. 그리고 pyjs라는 python 상에서 js함수를 실행해 반환해주는 라이브러리를 활용해 js함수를 실행해 필요한 데이터를 얻었다. 이제 재무제표가 담긴 URL까지 접근할 수 있으니 데이터를 스크래핑 하면 된다.

# 데이터 스크래퍼 개발

데이터를 스크래핑하는 것은 다소 지난한 과정이 예상되었다. 왜냐하면 재무제표의 형식이 조금씩 달랐기 떄문이다. 형식뿐만 아니라 계정 이름, 금액의 단위, 통화의 종류 등도 신경써주어야했다. 모든 재무제표에 일관적으로 적용되는 형식을 한 번에 찾아낼 수 없었다. 따라서 귀납적으로 일단 몇 개의 재무제표에 유효하게 기능을 개발하고, 점차적으로 더 많은 재무제표에 대해서도 유효하게 동작하도록 기능을 개선해가야했다. 그래서 다음과 같은 개발 로드맵을 세웠다.

    1. 20개의 재무제표 테스트 케이스를 통과하도록 개발
    2. Dart API에서 제공하는 데이터 수준에서 성능 검증 (여러 차례)
    3. 새로 발견된 결함 고려해 기능 개선

이 과정을 반복하다보니 대다수의 재무제표에 유효하게 작동하는 스크래퍼를 개발할 수 있었다. 최종본에 대해서 무작위 100개 기업에 대해서 2020~2022년 제출된 재무제표에 대해서 검증을 진행한 결과 95% 이상의 정확성을 보여줬다.

데이터 스크래퍼를 개발하면서 테스트의 힘을 제대로 느낄 수 있었다. 스크래퍼를 개발하면서 나중에 발견된 변수 때문에 스크래핑 기능의 세부구현을 다소 크게 수정하기도 했다. 만약 테스트가 없었다면 그런 수정이 있을 때마다 이미 앞서 유효하다고 판단한 케이스에 대해서도 잘 적용이 되는지 알 수 없어 개발이 매우 힘들었을 것이다. 심리적으로도 스트레스를 받았을 것이고, 잘못된 수정 하나로 오히려 성능이 매우 떨어질 수도 있었을 것이다. 하지만 테스트가 있다보니 큰 수정이 있어도 테스트를 모두 통과만 한다면 안십할 수 있었고, 만약 통과하지 못한 경우 다시 더 좋은 방식에 대해서 고민할 수 있었다. 소프트웨어를 적극적으로 개선하고, 더 좋은 방향으로 수정해나가기 위해서 테스트코드는 매우 중요하다.

# 멀티쓰레드로 데이터 스크래핑하기

파이썬은 I/O가 발생하면 쓰레드가 동기적으로 블락상태에 들어간다. 따라서 싱글스레드로 스크래핑을 하게되면 I/O로 인해 성능이 크게 저하된다. 따라서 멀리쓰레딩을 활용하여 데이터 스크래핑을 해주었다. 멀티쓰레딩을 적용했을 때 pyjs 라이브러리 관련 에러가 발생했다. pyjs 라이브러리는 멀티쓰레드 환경을 지원하지 않았다. 따라서 락을 활용해서 pyjs 라이브러리가 한 번에 하나의 쓰레드에 의해서만 접근되도록 처리해주었다.

# IP차단 문제

DART에 너무 빠르게 요청을 할 경우 IP가 차단당했다. 실험결과 0.5초 정도 간격을 두고 요청을 하면 문제가 없었다. 멀티 쓰레드에서 동시에 스크래핑을 하면서도 IP 차단을 당하지 않기 위해서 proxy 서버를 활용했다. stable proxy라는 곳에서 1.3달러 정도에 1주일 동안 쓸 수 있는 프록시 서버 10개 정도를 구매했다. 그리고 프록시 IP를 활용하여 10개의 쓰레드가 0.5초의 간격으로 스크래핑을 하도록 했다. 이렇게 하니까 IP 차단 문제가 발생하지 않았다.

# 스크래핑 진행 (1차, 2023.06.30)

## 예상

개발된 스크래퍼를 활용하여 상장기업의 2015년부터 2023년 1분기까지의 재무제표에 대해서 스크래핑을 진행했다. 2,356(상장 기업수) \* 33 = 77,748개의 레코드가 생길 것을 기대했다. 스크래핑 당 1초가 걸릴 것으로 예상했고, 10개의 쓰레드를 사용하였으므로 130분 정도 걸릴 것으로 생각했다.

## 결과

총 약 1,300분이 걸렸다. 예상보다 10배나 더 걸렸다. 재무제표 1개를 스크래핑하는데 1초가 아니라 10초가 걸린 것이다. 한 개 스크래핑을 할 때 http 요청이 2번 필요한데, 프록시를 통해 요청시 훨씬 오래 걸린다는 점을 간과한 결과였다. 프록시가 훨씬 오래 걸린다는 점을 고려했다면 IP 차단을 조심한다고 해도 타임슬립을 하지 않고 쓰레드를 5~6배 정도는 더 써도 괜찮았을 것 같다.

생성된 레코드의 개수는 61,557개이다. 2015년부터 재무제표가 없는 경우도 꽤나 많아서 예상보다 적은 레코드가 생긴것으로 보인다. 또한 중간중간 에러가 난 경우들이 있는 것으로 보인다. 추가적인 스크래핑을 진행하면서 스크래퍼 기능 개선을 계속하고 스크래핑 로직도 개선해야한다. 특히 에러가 왜 어떻게 난건지 보다 파악하기 쉽게 개선을 해야겠다.

스크래핑 결과를 보다보니 공시번호 20210817001851나 20230323001157같이 영업이익이 경 단위인 기업도 있었다. 확인해보니 단위가 원인데 백만원이라고 표기되어있는 경우였다. 이런 식의 에러도 종종 보여서 어느정도 수작업으로 데이터를 수정해주면서 데이터의 품질을 높여야 할 것 같다.

# 스크래핑 진행 (2차, 2023.08.06)

html을 저장하는 과정과 html에서 데이터를 스크래핑 하는 것을 두 단계로 나누어 진행했다. html을 요청하는 것이 IP 제한과 네트워크 I/O라서 오버헤드가 큰 작업이다. 따라서 한 번의 html을 저장해 놓고 쓰는 것이 추가적인 작업을 할 때 효율적이다.

금융감독원에서 제공하는 기업리스트에서 주식코드가 있는 기업만 뽑았다. 주식코드가 있는 기업의 2015년 4월 15일 이후부터 2023년 7월 28일까지 제출된 정기공시 보고서 목록을 Dart API로 요청해 파일로 저장했다. 기업 코드를 파일 이름으로하고 내용을 보고서 목록으로한 json 형식의 파일로 저장했다.

그렇게 json 형식으로 저장된 파일들로 부터 2023년 1분기 재무제표가 있는 기업만 뽑아냈다. 그렇게 하니까 총 2516개의 기업이 뽑혔다. (상장기업과 폐지했지만 정기공시를 하는 기업이 남아있었다.) 그리고 해당 기업들에 대해서 연결재무제표 html을 모두 저장했다. 정정보고의 경우 연결재무제표를 찾는데 어려움이 있는 경우가 소수 있어서 그것들에 대해서는 추가적인 스크립트를 작성해 html을 얻었다. 그렇게 총 69,765개의 연결재무제표 html을 얻었다. (연결재무제표가 없는 기업으로 데이터가 없는 html을 반환한 경우 포함)

확보한 연결재무제표 html을 대상으로 데이터 추출을 진행했다. 데이터가 없는 빈 html은 무시했다. DB에 54,381개의 레코드가 생겼다. 별개재무제표에 대해서도 동일하게 html를 확보한 후 스크래핑을 진행했다. 68,666개의 레코드가 생겼다. 연결재무제표의 경우 132 재무제표에 대해서, 별개의 경우 660개의 재무제표에 대해서 스크래핑에 실패했다.

실패한 케이스들을 테스트 케이스에 추가하면서 스크래퍼의 기능을 3차에 거처 개선했다. 개선 결과 스크래핑에 실패한 재무제표의 개수가 연결의 경우 19개, 별개의 경우 74개로 줄었다. 연결재무제표를 중심으로 테스트를 하며 스크래퍼를 개발했는데, 별개 재무제표의 경우 사업보고서에는 기간이 아예 표시되지 않는 경우도 있는 등 다소 다른 점이 있어서 실패한 케이스가 좀 더 많은 것으로 보인다.

데이터의 null 개수를 파악해보니 다음과 같았다.

```shell
연결재무제표 전체 레코드 개수와 컬럼 별 null 개수
+--------------------+-------+------------------+------------+--------+------+------------------+
| total_consolidated | sales | operating_profit | net_profit | equity | debt | cash_equivalents |
+--------------------+-------+------------------+------------+--------+------+------------------+
|              54427 |  1022 |               97 |       1300 |     99 |   79 |              517 |
+--------------------+-------+------------------+------------+--------+------+------------------+

별개재무제표 전체 레코드 개수와 컬럼 별 null 개수
+----------------+-------+------------------+------------+--------+------+------------------+
| total_seperate | sales | operating_profit | net_profit | equity | debt | cash_equivalents |
+----------------+-------+------------------+------------+--------+------+------------------+
|          68973 |  2263 |              126 |        979 |    101 |   92 |              651 |
+----------------+-------+------------------+------------+--------+------+------------------+
```

<!-- 2015년 1분기부터 2023년 1분기까지 데이터 스크래핑 마무리, +별개재무제표 맞춰 스크래퍼 기능 개선 + 수기로 데이터 채워주기(연결은 재무제표 자체 문제라 수기로 데이터 입력) and 2023년 2분기 데이터까지 스크래핑 진행-->
<!-- 현재 상장된 기업 데이터만 남기기(최종스크래핑대상상장기업목록.json), 현재는 DB에 상장 안되있는 기업 데이터도 좀 있음 -->
<!-- 더 과거 ~ 2014년 4분기까지 html 아카이빙 진행(현재 상장된 기업에 대해서만 진행하면 됨) 및 스크래핑 진행 -->
<!-- 스크래퍼 코드 리팩토링, 멀티쓰레딩 관련 코드 추상화(라이브러리화) 진행-->
