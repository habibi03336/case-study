# 큰 파일의 데이터 정렬하기 (외부정렬)

# 상황

한 줄에 문자열 하나가 써있는 10GB짜리 파일이 있다. 또 사용 가능한 메모리의 크기는 2GB이다. 이 파일을 어떻게 효율적으로 정렬할 수 있을까?

# 인식

파일이 10GB이고 사용 가능 메모리의 크기가 2GB이므로 모든 데이터를 메모리에 올려서 정렬을 하기는 어려운 상황이다.

따라서 데이터를 일부만 읽으면서도 전체를 정렬해야하는 다소 복잡한 상황이다.

# 접근

1. 가장 쉽게는 전체 파일을 반복적으로 n번 읽으면서 버블 정렬을 할 수 있다. 하지만 시간복잡도가 O(n^2)이다. (n: 문자열의 개수)

2. 파일을 특정 크기로 자르고, 각 부분을 정렬하여 파일로 저장한 후 병합정렬을 활용 할 수 있다.

3. 피벗보다 큰 값을 저장하는 파일, 작은 것을 저장하는 파일로 나누어서 퀵정렬을 활용 할 수 있다.

# 실험

## 설정

1. 한 줄에 하나의 문자열이 있는 1G짜리 파일을 준비한다. (원래 상황은 10G이지만 실험을 빠르게 진행하기 위해 1G로 줄였다.)

2. 메모리 크기는 200MB라고 가정한다.

## 방식 1. 버블 정렬

대략 1GB의 파일은 143,165,576개(약 1억 4300만개)의 문자를 가진다. 버블 정렬의 경우 O(n^2)(약 2경)이고, 더구나 I/O까지 계속 일어나기 때문에 현실적으로 쓰기 어려운 방식이다.

## 방식 2. 병합 정렬

1G의 파일은 200MB 씩 5개의 파일로 나눈다. 그리고 각 파트 별로 정렬하여 저장한다. 그리고 각각의 파일을 읽으며 병합정렬을 한다. O(nlogn) 시간복잡도를 가진다. 문제 조건 상 30억번 정도의 단위 연산이 일어난다고 할 수 있다. 빠르지는 않겠지만 충분히 실행가능한 수준이다.

실험 결과 정렬을 하는데 6분 13초 걸렸다. (이 결과를 바탕으로 버블정렬이 얼마의 시간이 걸릴지 역산해보면 80년이 넘게 나온다...ㅎㅎ)

## 방식 3. 퀵 정렬

랜덤으로 피벗을 하나 정한다. 피벗보다 작은 요소, 피벗보다 큰 요소를 나누어서 파일에 저장한다. 이를 재귀적으로 수행하다가 파일의 크기가 200MB보다 작아지면 메모리상에서 정렬하여 파일에 저장한다. 모든 파일이 정렬 완료되면 파일을 하나로 합친다.

실험 결과 정렬을 하는데 8분 7초가 걸렸다. 피벗에 따라서 성능이 달라질 수 있다.(실험시에 첫 피벗이 매우 치우쳐저 있었다.)

<div style="text-align: center;">
    <img src="https://raw.githubusercontent.com/habibi03336/case-study/master/2023-06-19-external-sorting/img/quick-recursion.png" alt="quick sort recursion" width="500"/>
</div>

---

    실험환경
    OS: macOS monterey
    CPU: Apple m1 pro
    Memory: 16GB
    파이썬: 3.9.13
